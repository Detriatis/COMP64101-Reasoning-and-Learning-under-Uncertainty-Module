Front: What is "generalisation" in deep learning?
Back: Performance on out-of-sample data (training or testing data).
Front: What should ideally hold for generalisation?
Back: Guarantees of predictive performance that hold at the distribution level.
Front: What are two key settings in generalisation?
Back: In-distribution and out-of-distribution
Front: What setting of generalisation has the same distribution for training, test, and future data?
Back: In-distribution generalisation
Front: What setting of generalisation has a different distribution for future data than for training/testing data?
Back: Out-of-distribution generalisation
Front: What is "mode connectivity"?
Back: The observation that independently trained SGD solutions can be connected by a curve of near-zero loss
Front: What does N_eff represent?
Back: Effective dimensionality of a model.
Front: What does Î»i represent in the equation for N_eff?
Back: Eigenvalues of the Hessian matrix.
Front: What did Zhang et al. (2017) find out about CNNs?
Back: They can fit CIFAR-10 with random labels but still generalize well on the clean test set.
Front: What kind of bounds are PAC-Bayes bounds?
Back: A kind of statistical learning bounds (a.k.a., generalization bounds, risk bounds).
Front: What does "certification" refer to in PAC-Bayes framework?
Back: Evaluating a bound value for a randomized classifier.
Front: What does "optimisation" refer to in PAC-Bayes framework?
Back: Using the bounds as a learning objective
