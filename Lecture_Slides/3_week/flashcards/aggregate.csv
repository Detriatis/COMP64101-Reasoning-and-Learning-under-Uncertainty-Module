" What is ""Monte Carlo Dropout""?"	 Randomly sampling after training by dropping out units according to a Bernoulli distribution.	
 What is SWA?	 Stochastic Weight Averaging	
 What are mixtures in deep ensembles?	 Coefficients are non-negative and add up to 1.	
 What are stacking in deep ensembles?	 Coefficients are non-negative but do not need to add up to 1.	
 Where can you find the material for Hierarchical Bayesian neural networks?	 Murphy (2023), pp. 675 - 678	
 Give three examples of techniques mentioned for Online inference	 Sequential Laplace, extended Kalman filtering, assumed density filtering, online variational inference	
" What is ""Monte Carlo Dropout""?"	 Randomly sampling after training by dropping out units according to a Bernoulli distribution.	
 What is SWA?	 Stochastic Weight Averaging	
 What are mixtures in deep ensembles?	 Coefficients are non-negative and add up to 1.	
 What are stacking in deep ensembles?	 Coefficients are non-negative but do not need to add up to 1.	
 Where can you find the material for Hierarchical Bayesian neural networks?	 Murphy (2023), pp. 675 - 678	
 Give three examples of techniques mentioned for Online inference	 Sequential Laplace, extended Kalman filtering, assumed density filtering, online variational inference	
 Why consider many possible parameter settings in BNNs, as opposed to only one, like in classical NNs?	 To improve accuracy and uncertainty representation, as many different settings can fit the data well but behave differently out-of-sample.	
 What are two key choices to consider when building a Bayesian neural network?	 Prior distribution and likelihood.	
 What is defined by choices of prior distribution and likelihood?	 The posterior distribution.	
" What does ""BNN"" stand for?"	 Bayesian Neural Network	
 What is a core principle of BNNs?	 To represent a distribution over functions instead of a single function.	
 Why are Bayesian methods sometimes preferred in deep learning?	 They are principled, generally interpretable, and may improve generalization.	
 Why can Bayesian methods be less used sometimes?	 Computational expense	
 What are the two main kinds of generalisation for BNNs?	 In-distribution and out-of-distribution generalisation	
 What is the main difference between in- and out-of-distribution generalisation?	 The future data is drawn from the training/testing distribution (in) or from another (out)	
" What does ""BNN"" stand for?"	 Bayesian Neural Network	
 What is a core principle of BNNs?	 To represent a distribution over functions instead of a single function.	
 Why are Bayesian methods sometimes preferred in deep learning?	 They are principled, generally interpretable, and may improve generalization.	
 Why can Bayesian methods be less used sometimes?	 Computational expense	
 What are the two main kinds of generalisation for BNNs?	 In-distribution and out-of-distribution generalisation	
 What is the main difference between in- and out-of-distribution generalisation?	 The future data is drawn from the training/testing distribution (in) or from another (out)	
" What is ""generalisation"" in deep learning?"	 Performance on out-of-sample data (training or testing data).	
 What should ideally hold for generalisation?	 Guarantees of predictive performance that hold at the distribution level.	
 What are two key settings in generalisation?	 In-distribution and out-of-distribution	
 What setting of generalisation has the same distribution for training, test, and future data?	 In-distribution generalisation	
 What setting of generalisation has a different distribution for future data than for training/testing data?	 Out-of-distribution generalisation	
" What is ""mode connectivity""?"	 The observation that independently trained SGD solutions can be connected by a curve of near-zero loss	
 What does N_eff represent?	 Effective dimensionality of a model.	
 What does Œªi represent in the equation for N_eff?	 Eigenvalues of the Hessian matrix.	
 What did Zhang et al. (2017) find out about CNNs?	 They can fit CIFAR-10 with random labels but still generalize well on the clean test set.	
 What kind of bounds are PAC-Bayes bounds?	 A kind of statistical learning bounds (a.k.a., generalization bounds, risk bounds).	
" What does ""certification"" refer to in PAC-Bayes framework?"	 Evaluating a bound value for a randomized classifier.	
" What does ""optimisation"" refer to in PAC-Bayes framework?"	 Using the bounds as a learning objective	
" What is ""generalisation"" in deep learning?"	 Performance on out-of-sample data (training or testing data).	
 What should ideally hold for generalisation?	 Guarantees of predictive performance that hold at the distribution level.	
 What are two key settings in generalisation?	 In-distribution and out-of-distribution	
 What setting of generalisation has the same distribution for training, test, and future data?	 In-distribution generalisation	
 What setting of generalisation has a different distribution for future data than for training/testing data?	 Out-of-distribution generalisation	
" What is ""mode connectivity""?"	 The observation that independently trained SGD solutions can be connected by a curve of near-zero loss	
 What does N_eff represent?	 Effective dimensionality of a model.	
 What does Œªi represent in the equation for N_eff?	 Eigenvalues of the Hessian matrix.	
 What did Zhang et al. (2017) find out about CNNs?	 They can fit CIFAR-10 with random labels but still generalize well on the clean test set.	
 What kind of bounds are PAC-Bayes bounds?	 A kind of statistical learning bounds (a.k.a., generalization bounds, risk bounds).	
" What does ""certification"" refer to in PAC-Bayes framework?"	 Evaluating a bound value for a randomized classifier.	
" What does ""optimisation"" refer to in PAC-Bayes framework?"	 Using the bounds as a learning objective	
 What is a neural network (NN)?	 A computational model defined by successive compositions of linear and non-linear functions.	
 What does a classical NN correspond to (graphically)?	 A graph consisting of nodes and edges linking some pairs of nodes.	
" What is a ""perceptron"" in the context of neural networks?"	 The basic building block of neural networks, performing a weighted sum with bias and applying an activation function.	
 What is the key linear operation in a neural network layer?	 An affine linear mapping, defined by a matrix and a bias vector.	
 What is the role of non-linear operations in neural network layers?	 They allow coordinate-wise application of a non-linear function to the output of the linear operation.	
 What do we stack in fully connected feedforward networks?	 Layers of linear and non-linear operations.	
" What are the ""trainable parameters"" of a neural network?"	 Entries of the matrices and bias vectors for all layers (i.e., weights).	
 What is another name for the classical fully connected feedforward NN?	 Multilayer Perceptron (MLP)	
 How are classical NNs trained?	 By optimizing parameters via Empirical Risk Minimization (ERM), using gradient descent on a loss function.	
 What is a neural network (NN)?	 A computational model defined by successive compositions of linear and non-linear functions.	
 What does a classical NN correspond to (graphically)?	 A graph consisting of nodes and edges linking some pairs of nodes.	
" What is a ""perceptron"" in the context of neural networks?"	 The basic building block of neural networks, performing a weighted sum with bias and applying an activation function.	
 What is the key linear operation in a neural network layer?	 An affine linear mapping, defined by a matrix and a bias vector.	
 What is the role of non-linear operations in neural network layers?	 They allow coordinate-wise application of a non-linear function to the output of the linear operation.	
 What do we stack in fully connected feedforward networks?	 Layers of linear and non-linear operations.	
" What are the ""trainable parameters"" of a neural network?"	 Entries of the matrices and bias vectors for all layers (i.e., weights).	
 What is another name for the classical fully connected feedforward NN?	 Multilayer Perceptron (MLP)	
 How are classical NNs trained?	 By optimizing parameters via Empirical Risk Minimization (ERM), using gradient descent on a loss function.	
 Write the formula for the posterior distribution in BNNs.	 p(w|D, Œ∏) = p(D|w, Œ∏)p(w|Œ∏) / p(D|Œ∏)	
 What is the numerator of the posterior formula?	 Likelihood times prior.	
 What is the denominator of the posterior formula?	 Evidence (normalization).	
 Write the formula for 'evidence', or the normalization term.	 p(D|Œ∏) = ‚à´ p(D|w, Œ∏)p(w|Œ∏) dw	
 In the case of supervised learning, what do we want to compute in BNNs?	 A posterior predictive distribution  p(y|x, D, Œ∏).	
 Name five methods for approximating posteriors in BNNs.	 Monte Carlo dropout, Laplace approximation, variational inference, expectation propagation, last layer methods.	
 Name five more methods for approximating posteriors in BNNs.	 SNGP, MCMC methods, SGD trajectory based methods, Deep ensembles, Approximating the posterior predictive distribution, Tempered and cold posteriors	
 Write the formula for the posterior distribution in BNNs.	 p(w|D, Œ∏) = p(D|w, Œ∏)p(w|Œ∏) / p(D|Œ∏)	
 What is the numerator of the posterior formula?	 Likelihood times prior.	
 What is the denominator of the posterior formula?	 Evidence (normalization).	
 Write the formula for 'evidence', or the normalization term.	 p(D|Œ∏) = ‚à´ p(D|w, Œ∏)p(w|Œ∏) dw	
 In the case of supervised learning, what do we want to compute in BNNs?	 A posterior predictive distribution  p(y|x, D, Œ∏).	
 Name five methods for approximating posteriors in BNNs.	 Monte Carlo dropout, Laplace approximation, variational inference, expectation propagation, last layer methods.	
 Name five more methods for approximating posteriors in BNNs.	 SNGP, MCMC methods, SGD trajectory based methods, Deep ensembles, Approximating the posterior predictive distribution, Tempered and cold posteriors	
 What is a good review on priors for BNNs?	" ""Priors in Bayesian Deep Learning  A Review"" by Vincent Fortuin et al"	
 What are the usual choices for priors of weights/biases?	 Gaussian distribution (can be a diagonal matrix to allow independent weights, and the mean usually set at 0)	
 What does nin refer to regarding weight initialization?	 Fan-in, number of nodes in previous layer	
 What does nout refer to regarding weight initialization?	 Fan-out, number of nodes in current layer	
 What does parameter vector ùúÉ represent in BNNs?	 It contains entries of all the matrices and bias vectors (i.e., weights) and variance parameters.	
 What are the Gaussian prior parameters called?	 Hyperparameters.	
 What is an alternative prior in BNNs other than Gaussian?	 Sparsity-promoting prior	
 What is a simplification in defining priors in BNNs?	 Covariance matrix is diagonal or a multiple of the identity	
" What is learned in ""empirical Bayes""?"	 Hyperparameters are optimized by maximizing the marginal likelihood.	
 What is a potential downside of cross-validation?	 It can be computationally expensive since it scales exponentially with the number of hyperparameters.	
 What kind of prior is involved in neural architecture search?	 Architectural priors or structural prior	
 What is a good review on priors for BNNs?	" ""Priors in Bayesian Deep Learning  A Review"" by Vincent Fortuin et al"	
 What are the usual choices for priors of weights/biases?	 Gaussian distribution (can be a diagonal matrix to allow independent weights, and the mean usually set at 0)	
 What does nin refer to regarding weight initialization?	 Fan-in, number of nodes in previous layer	
 What does nout refer to regarding weight initialization?	 Fan-out, number of nodes in current layer	
 What does parameter vector ùúÉ represent in BNNs?	 It contains entries of all the matrices and bias vectors (i.e., weights) and variance parameters.	
 What are the Gaussian prior parameters called?	 Hyperparameters.	
 What is an alternative prior in BNNs other than Gaussian?	 Sparsity-promoting prior	
 What is a simplification in defining priors in BNNs?	 Covariance matrix is diagonal or a multiple of the identity	
" What is learned in ""empirical Bayes""?"	 Hyperparameters are optimized by maximizing the marginal likelihood.	
 What is a potential downside of cross-validation?	 It can be computationally expensive since it scales exponentially with the number of hyperparameters.	
 What kind of prior is involved in neural architecture search?	 Architectural priors or structural prior	
